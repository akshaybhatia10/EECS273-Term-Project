{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7d0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import gensim.downloader as api\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from scipy.spatial.distance import cosine\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import utils\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a428e958",
   "metadata": {},
   "source": [
    "## Load the CCN Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b67f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ccnbook_ed4.txt', 'r') as f:\n",
    "    data = f.read()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9192f80e",
   "metadata": {},
   "source": [
    "## Test Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d9b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_list = ['dyslexia', 'dyslexia', 'dyslexia', 'dyslexia', 'dyslexia', 'dyslexia', 'added', 'summary', 'introducing', 'continuous', 'receptive']\n",
    "w2_list = ['reading problem', 'speech problem', 'speaking problem', 'reading', 'speech', 'speaking', 'adding', 'summarize', 'introduction', 'continuum', 'receptors']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4988299f",
   "metadata": {},
   "source": [
    "## Test Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0beb6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = ['neural activation function', 'transformation', 'bidirectional connectivity', 'cortex learning', 'object recognition','attention','weight based priming','hippocampus learning', 'dyslexia', 'past tense']\n",
    "answers = [['spiking rate code membrane potential point', 'interactive bidirectional feedforward', 'language generalization nonwords'], \n",
    "           ['emphasizing distinctions collapsing differences', 'error driven hebbian task model based', 'spiking rate code membrane potential point'],\n",
    "           ['amplification pattern completion','competition inhibition selection binding','language generalization nonwords'],\n",
    "           ['error driven task based hebbian model','error driven task based','gradual feature conjunction spatial invariance'],\n",
    "           ['gradual feature conjunction spatial invariance','error driven task based hebbian model','amplification pattern completion'],\n",
    "           ['competition inhibition selection binding','gradual feature conjunction spatial invariance','spiking rate code membrane potential point'],\n",
    "           ['long term changes learning','active maintenance short term residual','fast arbitrary details conjunctive'],\n",
    "           ['fast arbitrary details conjunctive','slow integration general structure','error driven hebbian task model based'],\n",
    "           ['surface deep phonological reading problem damage', 'speech output hearing language nonwords', 'competition inhibition selection binding'],\n",
    "           ['overregularization shaped curve', 'speech output hearing language nonwords', 'fast arbitrary details conjunctive']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce1ac10",
   "metadata": {},
   "source": [
    "## 1. Co-Occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678817f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def co_occurrence(sentences, window_size):\n",
    "    d = defaultdict(int)\n",
    "    vocab = set()\n",
    "    for text in sentences:\n",
    "        text = text.lower().split()\n",
    "        for i in range(len(text)):\n",
    "            token = text[i]\n",
    "            vocab.add(token)\n",
    "            next_token = text[i+1 : i+1+window_size]\n",
    "            for t in next_token:\n",
    "                key = tuple( sorted([t, token]) )\n",
    "                d[key] += 1\n",
    "    \n",
    "    vocab = sorted(vocab)\n",
    "    df = pd.DataFrame(data=np.zeros((len(vocab), len(vocab)), dtype=np.int16),\n",
    "                      index=vocab,\n",
    "                      columns=vocab)\n",
    "    for key, value in d.items():\n",
    "        df.at[key[0], key[1]] = value\n",
    "        df.at[key[1], key[0]] = value\n",
    "    return df, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0bf56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data.split('\\n')\n",
    "df, vocab = co_occurrence(sentences, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3055f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_vec(sentence, len_cutoff=1):\n",
    "    v = []\n",
    "    all_words = [word.lower() for word in sentence.split() if len(word) > len_cutoff] \n",
    "    for word in all_words:\n",
    "        if word in words:\n",
    "            v.append(list(df.loc[[word],:].values)[0])\n",
    "    v = np.array(v)\n",
    "    v = np.mean(v, 0)\n",
    "      \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7502d35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Words\n",
    "for word1, word2 in zip(w1_list, w2_list):\n",
    "    w1_vec, w2_vec = transform_to_vec(word1.lower()), transform_to_vec(word2.lower())\n",
    "    sim = 1 - cosine(w1_vec, w2_vec)\n",
    "    print (word1.lower() + ' vs ' + word2.lower() + ': ' + str(sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599584f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Quiz\n",
    "for q, ans in zip(ques, answers):\n",
    "    sims = []\n",
    "    ques_vec = transform_to_vec(q,)\n",
    "    print ('Question: ', q)\n",
    "    print ('Answers: ')\n",
    "    for a in ans:\n",
    "        ans_vec = transform_to_vec(a,)\n",
    "        sim = 1 - cosine(ques_vec, ans_vec)\n",
    "        sims.append(sim)\n",
    "        print (a, sim)\n",
    "    print (np.argmax(sims))\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71800d5",
   "metadata": {},
   "source": [
    "## 2. Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad3511",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "X = X.transpose().toarray()\n",
    "vocab = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b1500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_vec(sentence, len_cutoff=1):\n",
    "    v = []\n",
    "    all_words = [word.lower() for word in sentence.split() if len(word) > len_cutoff] \n",
    "    for word in all_words:\n",
    "        if word in words:\n",
    "            v.append(list(X[vocab[word]]))\n",
    "    v = np.array(v)\n",
    "    v = np.mean(v, 0)\n",
    "      \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05457cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Words\n",
    "for word1, word2 in zip(w1_list, w2_list):\n",
    "    w1_vec, w2_vec = transform_to_vec(word1.lower()), transform_to_vec(word2.lower())\n",
    "    sim = 1 - cosine(w1_vec, w2_vec)\n",
    "    print (word1.lower() + ' vs ' + word2.lower() + ': ' + str(sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3394f03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Quiz\n",
    "for q, ans in zip(ques, answers):\n",
    "    sims = []\n",
    "    ques_vec = transform_to_vec(q,)\n",
    "    print ('Question: ', q)\n",
    "    print ('Answers: ')\n",
    "    for a in ans:\n",
    "        ans_vec = transform_to_vec(a,)\n",
    "        sim = 1 - cosine(ques_vec, ans_vec)\n",
    "        sims.append(sim)\n",
    "        print (a, sim)\n",
    "    print (np.argmax(sims))\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763df687",
   "metadata": {},
   "source": [
    "## 3. word2vec general "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ca251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6fe001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_vec(sentence, len_cutoff=1):\n",
    "    vectors = []\n",
    "    \n",
    "    for doc in sentence:\n",
    "        v = []\n",
    "        words = [word.lower() for word in sentence.split() if len(word) > len_cutoff] \n",
    "        for word in words:\n",
    "            if word in wv:\n",
    "                v.append(wv[word])\n",
    "        v = np.array(v)\n",
    "        v = np.mean(v, 0)\n",
    "      \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88702c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Words\n",
    "for word1, word2 in zip(w1_list, w2_list):\n",
    "    w1_vec, w2_vec = transform_to_vec(word1.lower()), transform_to_vec(word2.lower())\n",
    "    sim = 1 - cosine(w1_vec, w2_vec)\n",
    "    print (word1.lower() + ' vs ' + word2.lower() + ': ' + str(sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fc5aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Quiz\n",
    "for q, ans in zip(ques, answers):\n",
    "    sims = []\n",
    "    ques_vec = transform_to_vec(q)\n",
    "    print ('Question: ', q)\n",
    "    print ('Answers: ')\n",
    "    for a in ans:\n",
    "        ans_vec = transform_to_vec(a)\n",
    "        sim = 1 - cosine(ques_vec, ans_vec)\n",
    "        sims.append(sim)\n",
    "        print (a, sim)\n",
    "    print (np.argmax(sims))\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ba7978",
   "metadata": {},
   "source": [
    "## 4. word2vec custom training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260f954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = utils.preprocess(data)\n",
    "print(\"Total words: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words))))\n",
    "vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b859dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f6a407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    \n",
    "    return list(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7695be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    \n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5b6281",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf0a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(int_to_vocab)\n",
    "n_embedding = 300 # Number of embedding features \n",
    "with train_graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d0ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of negative labels to sample\n",
    "n_sampled = 1000\n",
    "with train_graph.as_default():\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding), stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(n_vocab))\n",
    "    \n",
    "    # Calculate the loss using negative sampling\n",
    "    loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, \n",
    "                                      labels, embed,\n",
    "                                      n_sampled, n_vocab)\n",
    "    \n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8344b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    ## From Thushan Ganegedara's implementation\n",
    "    valid_size = 64 # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100\n",
    "    # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, \n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "    normalized_embedding = embedding / norm\n",
    "    valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir checkpoints\n",
    "window_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8feea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "batch_size = 1024\n",
    "window_size = 10\n",
    "\n",
    "train = False\n",
    "if train:\n",
    "    with train_graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        iteration = 1\n",
    "        loss = 0\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(1, epochs+1):\n",
    "            batches = get_batches(train_words, batch_size, window_size)\n",
    "            start = time.time()\n",
    "            for x, y in batches:\n",
    "\n",
    "                feed = {inputs: x,\n",
    "                        labels: np.array(y)[:, None]}\n",
    "                train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "\n",
    "                loss += train_loss\n",
    "\n",
    "                if epoch % 1000 == 0: \n",
    "                    end = time.time()\n",
    "                    print(\"Epoch {}/{}\".format(epoch, epochs),\n",
    "                          \"Iteration: {}\".format(iteration),\n",
    "                          \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                          \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                    loss = 0\n",
    "                    start = time.time()\n",
    "\n",
    "                if epoch % 1000 == 0:\n",
    "                    # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "                    sim = similarity.eval()\n",
    "                    for i in range(valid_size):\n",
    "                        valid_word = int_to_vocab[valid_examples[i]]\n",
    "                        top_k = 8 # number of nearest neighbors\n",
    "                        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                        log = 'Nearest to %s:' % valid_word\n",
    "                        for k in range(top_k):\n",
    "                            close_word = int_to_vocab[nearest[k]]\n",
    "                            log = '%s %s,' % (log, close_word)\n",
    "                        print(log)\n",
    "\n",
    "                iteration += 1\n",
    "            save_path = saver.save(sess, \"checkpoints/ccn.ckpt\")\n",
    "            embed_mat = sess.run(normalized_embedding)\n",
    "else:\n",
    "    with train_graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session(graph=train_graph) as sess:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "        embed_mat = sess.run(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b2df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_vec(sentence, len_cutoff=1):\n",
    "    vectors = []\n",
    "    \n",
    "    for doc in sentence:\n",
    "        v = []\n",
    "        words = [word.lower() for word in sentence.split() if len(word) > len_cutoff] \n",
    "        for word in words:\n",
    "            if word in vocab_to_int:\n",
    "                v.append(embed_mat[vocab_to_int[word]])\n",
    "        v = np.array(v)\n",
    "        v = np.mean(v, 0)\n",
    "      \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b2b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test words\n",
    "for word1, word2 in zip(w1_list, w2_list):\n",
    "    w1_vec, w2_vec = transform_to_vec(word1.lower()), transform_to_vec(word2.lower())\n",
    "    sim = 1 - cosine(w1_vec, w2_vec)\n",
    "    print (word1.lower() + ' vs ' + word2.lower() + ': ' + str(sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea37db86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Quiz\n",
    "for q, ans in zip(ques, answers):\n",
    "    sims = []\n",
    "    ques_vec = transform_to_vec(q)\n",
    "    print ('Question: ', q)\n",
    "    print ('Answers: ')\n",
    "    for a in ans:\n",
    "        ans_vec = transform_to_vec(a)\n",
    "        sim = 1 - cosine(ques_vec, ans_vec)\n",
    "        sims.append(sim)\n",
    "        print (a, sim)\n",
    "    print (np.argmax(sims))\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d108d",
   "metadata": {},
   "source": [
    "## 5. BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45b2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aad7d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(text):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                    output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                    )\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings.size()\n",
    "\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings.size()\n",
    "\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    token_vecs_cat = []\n",
    "\n",
    "    for token in token_embeddings:\n",
    "        cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "        token_vecs_cat.append(cat_vec)\n",
    "\n",
    "    token_vecs_sum = []\n",
    "    for token in token_embeddings:\n",
    "        sum_vec = torch.sum(token[-4:], dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "    s = torch.mean(token_vecs, dim=0)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf2ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test words\n",
    "for word1, word2 in zip(w1_list, w2_list):\n",
    "    w1_vec, w2_vec = sentence_embedding(word1.lower()), sentence_embedding(word2.lower())\n",
    "    sim = 1 - cosine(w1_vec, w2_vec)\n",
    "    print (word1.lower() + ' vs ' + word2.lower() + ': ' + str(sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2f04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Quiz\n",
    "for q, ans in zip(ques, answers):\n",
    "    sims = []\n",
    "    ques_vec = sentence_embedding(q)\n",
    "    print ('Question: ', q)\n",
    "    print ('Answers: ')\n",
    "    for a in ans:\n",
    "        ans_vec = sentence_embedding(a)\n",
    "        sim = 1 - cosine(ques_vec, ans_vec)\n",
    "        sims.append(sim)\n",
    "        print (a, sim)\n",
    "    print (np.argmax(sims))\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3528faf",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c08208",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db70fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_words = 500\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c186b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
